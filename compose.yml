---
services:
  mealie-mcp:
    image: docker.io/knucklessg1/mealie:latest
    #    build:
    #      context: . # Debug
    #      dockerfile: debug.Dockerfile
    container_name: mealie-mcp
    hostname: mealie-mcp
    command: [ "mealie-mcp" ]
    extra_hosts:
      - "host.docker.internal:host-gateway"
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
    restart: always
    environment:
      - "HOST=0.0.0.0"
      - "PORT=8013"
      - "TRANSPORT=streamable-http"
      - "MEALIE_BASE_URL=${MEALIE_BASE_URL}"
      - "MEALIE_TOKEN=${MEALIE_TOKEN}"
      - "MEALIE_VERIFY=${MEALIE_VERIFY:-True}"
    ports:
      - "8013:8013"
    healthcheck:
      test: [ "CMD", "python3", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:8013/health')" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  mealie-agent:
    image: docker.io/knucklessg1/mealie:latest
    #    build:
    #      context: . # Debug
    #      dockerfile: debug.Dockerfile
    container_name: mealie-agent
    hostname: mealie-agent
    command: [ "mealie-agent" ]
    extra_hosts:
      - "host.docker.internal:host-gateway"
    depends_on:
      - mealie-mcp
    logging:
      driver: json-file
      options:
        max-size: "10m"
        max-file: "3"
    restart: always
    env_file:
      - .env
    environment:
      - "HOST=0.0.0.0"
      - "PORT=9013"
      - "MCP_URL=http://mealie-mcp:8013/mcp"
      - "PROVIDER=openai"
      - "LLM_BASE_URL=${LLM_BASE_URL:-http://host.docker.internal:1234/v1}"
      - "LLM_API_KEY=${LLM_API_KEY:-llama}"
      - "MODEL_ID=${MODEL_ID:-qwen/qwen3-coder-next}"
      - "DEBUG=False"
      - "ENABLE_WEB_UI=True"
      - "ENABLE_OTEL=True"
    ports:
      - "9013:9013"
    healthcheck:
      test: [ "CMD", "python3", "-c", "import urllib.request; urllib.request.urlopen('http://localhost:9013/health')" ]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s
